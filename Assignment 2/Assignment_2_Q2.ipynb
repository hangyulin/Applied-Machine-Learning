{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greek</td>\n",
       "      <td>10259</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>25693</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filipino</td>\n",
       "      <td>20130</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>indian</td>\n",
       "      <td>22213</td>\n",
       "      <td>[water, vegetable oil, wheat, salt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indian</td>\n",
       "      <td>13162</td>\n",
       "      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cuisine     id                                        ingredients\n",
       "0        greek  10259  [romaine lettuce, black olives, grape tomatoes...\n",
       "1  southern_us  25693  [plain flour, ground pepper, salt, tomatoes, g...\n",
       "2     filipino  20130  [eggs, pepper, salt, mayonaise, cooking oil, g...\n",
       "3       indian  22213                [water, vegetable oil, wheat, salt]\n",
       "4       indian  13162  [black pepper, shallots, cornflour, cayenne pe..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading What's Cooking data into the file \n",
    "train_data_Cooking = pd.read_json('train.json')\n",
    "test_data_Cooking = pd.read_json('test.json')\n",
    "train_data_Cooking.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20 unique cuisines and 6714 unique ingredients\n"
     ]
    }
   ],
   "source": [
    "#find the number of samples in the training set \n",
    "train_data_size = train_data_Cooking.shape\n",
    "#find the number of type of cuisines in the training set \n",
    "type_of_cuisine = train_data_Cooking.cuisine.nunique()\n",
    "#create a list that keeps all the ingredients\n",
    "all_ingredients = np.array(train_data_Cooking[\"ingredients\"].values)\n",
    "#create a list that has all the unique ingredients\n",
    "unique_ingredients = np.unique(np.hstack(all_ingredients))\n",
    "#calculates the number of unique ingredients \n",
    "num_unique_ingredients = len(unique_ingredients)\n",
    "print(\"There are\", type_of_cuisine, \"unique cuisines and\", num_unique_ingredients, \"unique ingredients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are 39774 dishes in the training set, 20 types of cuisines and 6714 unique ingredients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_data_Cooking['cuisine'].values\n",
    "train_data = train_data_Cooking.drop(['cuisine','id'], axis = 1)\n",
    "test_id = test_data_Cooking['id'].values\n",
    "test_data = test_data_Cooking.drop(['id'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoder to convert the labels into vectors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Convert the data into vectors\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(    oz.) tomato sauce</th>\n",
       "      <th>(   oz.) tomato paste</th>\n",
       "      <th>(10 oz.) frozen chopped spinach</th>\n",
       "      <th>(10 oz.) frozen chopped spinach, thawed and squeezed dry</th>\n",
       "      <th>(14 oz.) sweetened condensed milk</th>\n",
       "      <th>(14.5 oz.) diced tomatoes</th>\n",
       "      <th>(15 oz.) refried beans</th>\n",
       "      <th>1% low-fat buttermilk</th>\n",
       "      <th>1% low-fat chocolate milk</th>\n",
       "      <th>1% low-fat cottage cheese</th>\n",
       "      <th>...</th>\n",
       "      <th>yukon gold potatoes</th>\n",
       "      <th>yuzu</th>\n",
       "      <th>yuzu juice</th>\n",
       "      <th>za'atar</th>\n",
       "      <th>zest</th>\n",
       "      <th>zesty italian dressing</th>\n",
       "      <th>zinfandel</th>\n",
       "      <th>ziti</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zucchini blossoms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6714 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   (    oz.) tomato sauce  (   oz.) tomato paste  \\\n",
       "0                       0                      0   \n",
       "1                       0                      0   \n",
       "2                       0                      0   \n",
       "3                       0                      0   \n",
       "4                       0                      0   \n",
       "\n",
       "   (10 oz.) frozen chopped spinach  \\\n",
       "0                                0   \n",
       "1                                0   \n",
       "2                                0   \n",
       "3                                0   \n",
       "4                                0   \n",
       "\n",
       "   (10 oz.) frozen chopped spinach, thawed and squeezed dry  \\\n",
       "0                                                  0          \n",
       "1                                                  0          \n",
       "2                                                  0          \n",
       "3                                                  0          \n",
       "4                                                  0          \n",
       "\n",
       "   (14 oz.) sweetened condensed milk  (14.5 oz.) diced tomatoes  \\\n",
       "0                                  0                          0   \n",
       "1                                  0                          0   \n",
       "2                                  0                          0   \n",
       "3                                  0                          0   \n",
       "4                                  0                          0   \n",
       "\n",
       "   (15 oz.) refried beans  1% low-fat buttermilk  1% low-fat chocolate milk  \\\n",
       "0                       0                      0                          0   \n",
       "1                       0                      0                          0   \n",
       "2                       0                      0                          0   \n",
       "3                       0                      0                          0   \n",
       "4                       0                      0                          0   \n",
       "\n",
       "   1% low-fat cottage cheese        ...          yukon gold potatoes  yuzu  \\\n",
       "0                          0        ...                            0     0   \n",
       "1                          0        ...                            0     0   \n",
       "2                          0        ...                            0     0   \n",
       "3                          0        ...                            0     0   \n",
       "4                          0        ...                            0     0   \n",
       "\n",
       "   yuzu juice  za'atar  zest  zesty italian dressing  zinfandel  ziti  \\\n",
       "0           0        0     0                       0          0     0   \n",
       "1           0        0     0                       0          0     0   \n",
       "2           0        0     0                       0          0     0   \n",
       "3           0        0     0                       0          0     0   \n",
       "4           0        0     0                       0          0     0   \n",
       "\n",
       "   zucchini  zucchini blossoms  \n",
       "0         0                  0  \n",
       "1         0                  0  \n",
       "2         0                  0  \n",
       "3         0                  0  \n",
       "4         0                  0  \n",
       "\n",
       "[5 rows x 6714 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use one hot encoder to transform categorical variables to binary feature vectors\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_data_clean = train_data.join(pd.DataFrame(mlb.fit_transform(train_data.pop('ingredients')),columns = mlb.classes_, index = train_data.index))\n",
    "train_data_clean.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hangyulin/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['2% reduced fat chocolate milk', '33% less sodium smoked ham', '50% less sodium black beans', 'Asian herb', 'Best Foods Mayonnaise Dressing with Extra Virgin Olive Oil', 'Boursin Cheese with Garlic and Herbs', 'Bramley apples', 'CURRY GUY Smoked Tandoori Masala', 'Chobani Yogurt', 'Crisco Canola Oil', 'Daisy Brand Light Sour Cream', 'Del Monte Diced Tomatoes', 'Domaine de Canton Ginger Liqueur', 'Dreamfields Lasagna', 'Dubliner cheese', 'Duncan Hines Classic White Cake Mix', 'Foster Farms chicken drumsticks', 'Franks Wings Sauce', \"French's Spicy Brown Mustard\", 'Goya Corn Oil', 'Goya Seasoning', 'Green Giant™ Steamers™ Niblets® frozen corn', 'Green Giant™ frozen chopped spinach', 'Grey Poupon Dijon Mustard', \"Hellmann's Mayonnaise with a hint of Wasabi\", 'Hershey bars', 'Holland House White Cooking Wine', 'Honey Bunches of Oats Cereal', 'Johnsonville® Italian All Natural Hot Ground Sausage', 'KRAFT Classic Ranch Dressing', 'Kettle Chips', 'King Arthur Gluten Free MultiPurpose Flour', 'Klondike Gourmet mini potatoes', 'Knorr® Beef flavored Bouillon Cube', 'Knorr® Chicken Flavor Rice Sides™', 'Kraft Pepper Jack', 'Lipton® Cup Size Tea Bags', 'Malbec', 'McCormick Black Pepper', 'McCormick Chili Powder', 'McCormick Garlic Powder', 'McCormick Oregano Leaves', 'McCormick Original Country Gravy Mix', 'Mountain High Yoghurt', 'Nu-Salt Salt Substitute', 'Ortega Taco Seasoning', 'Pace Salsa', 'Pillsbury Biscuits', 'Pillsbury™ refrigerated garlic butter crescent dinner rolls', 'Pompeian Extra Virgin Olive Oil', 'Ronzoni Penne Rigate', 'Sargento® Artisan Blends® Shredded Whole Milk Mozzarella Cheese', 'Sargento® Traditional Cut Shredded Extra Sharp Cheddar Cheese', 'Silk Original Soymilk', 'Simply Organic Cinnamon', 'Simply Organic ground nutmeg', 'Simply Organic® sea salt', 'Spanish tuna', 'Special K Cereal', 'Spice Islands Chili Powder', 'Spice Islands Garlic Powder', 'Spice Islands® Chipotle Chile Powder', 'TACO BELL® Thick & Chunky Medium Salsa', 'Thai Kitchen Fish Sauce', 'Thai Kitchen Red Curry Paste', 'Tuttorosso Peeled Plum Shaped Tomatoes', \"Uncle Ben's® Ready Rice® Original Long Grain\", 'V8 100% Vegetable Juice', 'Yakisoba sauce', 'Yoplait® Greek 2% Key lime pie yogurt', 'activ dry quick rise yeast', 'alaskan halibut', 'alfredostyle pasta sauce', 'alouette', 'amaranth seeds', 'american eggplant', 'american long grain rice', 'anardana', 'angelica', 'anise flavoring', 'asian chile sauce with garlic', 'ataulfo', 'baby octopus', 'baked potato', 'banana chips', 'banana pudding', 'banh trang', 'base sauce', 'beef fat', 'beef strips', 'beluga caviar', 'bertolli olive oil & garlic sauc', 'bertolli vidalia onion with roast garlic sauc', 'bitter chocolate', 'bitter orange juice', 'black chickpeas', 'black currant', 'black quinoa', 'black sticky rice', 'blended whiskey', 'blueberry jam', 'bluefish', 'boiler', 'boneless beef bottom round roast', 'boneless beef top round steak', 'boneless flank steak', 'boneless moulard duck breast', 'bottled peperoncini', 'bouillon shrimp', 'brains', 'braised seitan', 'breaded chicken fillets', 'breakfast pork sausage', 'broccoli sprouts', 'brown dark firmli pack sugar', 'brown gravy seasoning mix', 'buffalo meat', 'butter cake mix', 'butter cookies', 'butterhead lettuce', 'buttery spread', 'caciotta', 'cal', 'candied flowers', 'candied mixed citrus peel', 'carcass', 'cassis', 'celery cabbage', 'cepe', 'chablis', 'chai tea', 'chamomile tea', 'chapati', 'chaurice', 'chees fat grate parmesan reduc', 'cheese croutons', 'cherry brandy', 'chestnut honey', 'chicken consomme soup mix', 'chicken leg with thigh', 'chicken seasoning mix', 'chili leaf', 'chili purée', 'chive flowers', 'chocolate cookie', 'cholent', 'chopped macadamias', 'chuck eye steak', 'citrus rind', 'coleslaw seasoning blend', 'cooked cut green beans', 'cooked seafood', 'cooking greens', 'corncobs', 'cotechino', 'crab leg', 'cracked green olives', 'cracked peppercorn', 'crawfish fat', 'cream lowfat', 'cremini caps', 'crisco shortening', 'crispy chow mein noodles', 'crumble topping', 'crushed ritz crackers', 'cubed pork', 'cupcakes', 'cured beef', 'diced beef', 'diced candied citron', 'diet lemon lime soda', 'diet orange soda', 'dill pickle spear', 'dogfish', 'dream whip', 'dress russian', 'dried cilantro leaves', 'dried ziti', 'elderflower syrup', 'emerils essence', 'enchilada seasoning', 'farfallini', 'fat free instant chocolate pudding mix', 'fat free ranch dressing', 'fat-free italian salad dressing', 'fen szu', 'fish bouillon cube', 'flowerets', 'foie gras medallions', 'fondant', 'fraise', 'fresh', 'fresh gnocchi', 'fresh ham butt', 'fresh seafood', 'frozen Italian blend vegetables', 'frozen chicken wings', 'frozen cranberries', 'frozen peeled prawns', 'gammon', 'ginger garlic stir fry sauce', 'glazed doughnuts', 'gluten free rice chex', 'gluten-free pizza crust', 'golden cake mix', 'golden pineapple', 'grains of paradise', 'grape jelly', 'grated sharp cheese', 'ground cacao', 'ground flax', 'ground lemongrass', 'ground pork fat', 'gumbo base', 'gumdrops', 'ham stock cube', 'hazelnut paste', 'herb mix', 'herb sauce', 'herbal tea', 'honey gold potatoes', 'horse gram', 'horseradish cream', 'hot cross buns', 'hot dog rolls', 'hushpuppy mix', 'indian bay leaf', 'italian moscato', 'italian sandwich rolls', 'italian spicy sausage', 'japanese greens', 'japanese noodles', 'jasmine flowers', 'kataifi', 'knorr shrimp flavor bouillon cube', 'knox gelatin powder', 'konbu dashi', 'krachai', 'la', 'large potatoes', 'laughing cow', 'lean meat', 'lemon flavor instant pudding mix', 'lemon flavor yogurt', 'lettuce hearts', 'licorice', 'light syrup', 'light tuna in oil', 'liquid egg whites', 'liquid non-dairy creamer', 'liquid stevia', 'long green chili peppers', 'loose black tea', 'lovage', 'low fat small curd cottag chees', 'low sodium canned vegetable stock', 'low sodium creole seasoning', 'low sodium mozzarella cheese', 'low sodium pasta sauce', 'low sodium turkey breast', 'low sodium vegetable juice cocktail', 'low-fat chocolate ice cream', 'low-fat creamy peanut butter', 'low-fat granola', 'low-fat italian dressing', 'low-fat tomato-and-basil pasta sauc', 'low-fat whole wheat crackers', 'low-fat whole wheat tortillas', 'manzanilla sherry', 'mccormick perfect pinch italian seasoning', 'medium grain brown rice', 'merguez', 'meringue shells', 'mild white fish', 'mini filo tartlet shells', 'mint jelly', 'molasses sugar', \"moscato d'asti\", 'multigrain bread', 'mushroom marinara', 'mushroom sauce', 'nama yuba', 'natural vanilla extract', 'nen dzem fen', 'new mexican chile', 'no salt added ketchup', 'no-chicken broth', 'non-fat soymilk', 'nonfat beef broth', 'nonfat thousand island dressing', 'nut butter', 'olivada', 'opal basil', 'orange lentils', 'panch phoron', 'passover cake meal', 'pasteurized process cheese spread', 'pastrami', 'pastry for single crust pie', 'peach jam', 'peach yogurt', 'pepper vodka', 'pesto sauce mix', 'phyllo sheets', 'pie pumpkin', \"pig's ear\", 'pinipig', 'pomegranate syrup', 'popsicle', 'pork steaks', 'pork strips', 'pork top loin chops', 'potassium carbonate', 'potato puree', 'potato slider buns', 'potato soup', 'potato sticks', 'praline paste', 'preserved black winter truffles', 'pretzels', 'promis spread stick', 'pumpkin butter', 'pumpkin pie filling', 'purple bell peppers', 'queso ranchero', 'quick rolled oats', 'quick-cooking oatmeal', 'ragu old world style smooth pasta sauc', 'rapini', 'recaito', 'red horseradish', 'red rice vinegar', 'reduced fat reduced sodium condensed cream of chicken soup', 'reduced fat reduced sodium swiss cheese', 'reduced sodium tomato juice', 'reduced sodium vegetable juice', 'ridged ziti', 'risotto mix', 'roast beef fat', 'rock candy syrup', 'rose extract', 'rose leaves', 'rose syrup', 'rouget', 'rum syrup', 'rye meal', 'salsa seasoning mix', 'sambhar powder', 'sandwich cookies', 'seedless oranges', 'sevruga caviar', 'shad fillets', 'shanghai-style noodles', 'shark fillets', 'shelled prawn', 'shredded low-fat cheese', 'shredded phyllo dough', 'sicilian', 'skin on salmon fillets', 'skin-on cod fillets', 'skinless boneless chicken legs', 'skinless boneless duck breast halves', 'skinless smoked trout fillets', 'small yellow potatoes', 'smooth pasta', 'sodium', 'soft cheese', 'soup noodles', 'sour pickle', 'soy paste', 'soy vermicelli', 'spiced pecans', 'splenda granulated', 'spring chicken', 'spring salad mix', 'squirrel', 'star fruit', 'steamed bun flour', 'stir fry noodles', 'stir fry oil', 'streusel topping', 'sugar cookie mix', 'surimi', 'sweet potato squash', 'tabbouleh', 'taco rub', 'taglierini', 'tarragon sprigs', 'tat soi', 'thai ginger', 'thai tea leaves', 'toffee pieces', 'tofu mayonnaise', 'tomato basil feta', 'tomato coulis', 'tonic', 'tortilla bowls', 'trenne', 'tri color pasta twists, cooked and drained', 'turkey chili with beans', 'unagi sauce', 'uncooked lasagna', 'unsalted creamy peanut butter', 'vegan milk substitute', 'vegetable base', 'vienna sausage', 'vietnamese spinach', 'wafer cookies', 'whisky', 'white frosting mix', 'white lentils', 'whiting', 'whole green peperoncini', 'whole wheat cheese tortellini', 'whole wheat spiral pasta', 'whole wheat white flour', 'whole wheat wraps', 'wish-bone deluxe french dressing', 'yellow chile', 'yellow rice seasoning mix', 'yodel', 'young ginger', 'zabaglione'] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    }
   ],
   "source": [
    "# Transform the test data too\n",
    "mlb_2 = MultiLabelBinarizer()\n",
    "test_data_clean = test_data.join(pd.DataFrame(mlb.transform(test_data.pop('ingredients')),columns = mlb.classes_, index = test_data.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the label too\n",
    "label_enc = LabelEncoder()\n",
    "train_label_vec = label_enc.fit_transform(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into numpy array\n",
    "train_data_vec = train_data_clean.values\n",
    "test_data_vec = test_data_clean.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pipeline to run model\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Using gridsearch to search for best hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian distribution Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=GaussianNB(priors=None, var_smoothing=1e-09),\n",
       "       fit_params=None, iid='warn', n_jobs=None, param_grid=[{}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Gaussian distribution prior assumption for Naive Bayes Classifier\n",
    "grid_param_gnb = [{}]\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "grid_search_gnb = GridSearchCV(gnb, grid_param_gnb, cv = 3,\n",
    "                           scoring='accuracy', return_train_score=True)\n",
    "\n",
    "grid_search_gnb.fit(train_data_vec, train_label_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for Gaussian distribution prior assumption Naïve Bayes Classifier is 0.379494\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy for Gaussian distribution prior assumption Naïve Bayes Classifier is\", round(grid_search_gnb.best_score_,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli distribution Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid=[{'alpha': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Bernoulli distribution prior assumption for Naive Bayes Classifier\n",
    "grid_param_bnb = [{'alpha': np.arange(0.1, 1, 0.1)}]\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "grid_search_bnb = GridSearchCV(bnb, grid_param_bnb, cv = 3,\n",
    "                           scoring='accuracy', return_train_score=True)\n",
    "\n",
    "grid_search_bnb.fit(train_data_vec, train_label_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for Bernoulli distribution prior assumption Naïve Bayes Classifier is 0.744959 with best alpha of {'alpha': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy for Bernoulli distribution prior assumption Naïve Bayes Classifier is\", round(grid_search_bnb.best_score_,6),\n",
    "      \"with best alpha of\", grid_search_bnb.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Accuracy is \n",
    "Bernoulli Accuracy is \n",
    "From 3-fold cross validation,  performs better "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing logistic regression model from sklearn \n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=4000, multi_class='auto',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid=[{'C': array([0.5 , 0.75, 1.  , 1.25, 1.5 , 1.75])}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Gaussian distribution prior assumption for Naive Bayes Classifier\n",
    "grid_param_lg = [{\"C\": np.arange(0.5, 2, 0.25)}]\n",
    "\n",
    "lg = LogisticRegression(solver='lbfgs', multi_class='auto', penalty = \"l2\", max_iter = 4000)\n",
    "\n",
    "grid_search_lg = GridSearchCV(lg, grid_param_lg, cv = 3,\n",
    "                           scoring='accuracy', return_train_score=True)\n",
    "\n",
    "grid_search_lg.fit(train_data_vec, train_label_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for Bernoulli distribution prior assumption Naïve Bayes Classifier is 0.772917 with best alpha of {'C': 0.75}\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy for Bernoulli distribution prior assumption Naïve Bayes Classifier is\", round(grid_search_lg.best_score_,6),\n",
    "      \"with best alpha of\", grid_search_lg.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# g)\n",
    "The best model was the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use linear regression to predict the test data\n",
    "test_pred = grid_search_lg.best_estimator_.predict(test_data_vec)\n",
    "test_pred_label = label_enc.inverse_transform(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cuisine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18009</td>\n",
       "      <td>british</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28583</td>\n",
       "      <td>southern_us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41580</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29752</td>\n",
       "      <td>cajun_creole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35687</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id       cuisine\n",
       "0  18009       british\n",
       "1  28583   southern_us\n",
       "2  41580       italian\n",
       "3  29752  cajun_creole\n",
       "4  35687       italian"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pd = pd.DataFrame({\"id\":test_id, \"cuisine\":test_pred_label})\n",
    "test_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pd.to_csv(\"Question2_submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "lgr = LogisticRegression(solver='lbfgs', multi_class='auto', penalty = \"l2\", max_iter = 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipe = Pipeline(steps=[(\"PCA\", pca),(\"LGR\", lgr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'PCA__n_components': [1500],\n",
    "    'LGR__C': np.arange(0.7, 0.9, 0.1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_grid = GridSearchCV(model_pipe, param_grid, cv = 3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hangyulin/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-2d52711d8e4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msuper_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    853\u001b[0m                     \u001b[0;31m# scheduling.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransportableException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mabort_everything\u001b[0;34m(self, ensure_ready)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \"\"\"Shutdown the workers and restart a new one with the same parameters\n\u001b[1;32m    537\u001b[0m         \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         \u001b[0mdelete_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temp_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, kill_workers)\u001b[0m\n\u001b[1;32m   1094\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m                 \u001b[0mqmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0mcq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_queue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "super_grid.fit(train_data_vec, train_label_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
